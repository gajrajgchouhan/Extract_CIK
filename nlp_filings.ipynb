{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gajrajgchouhan/Extract_CIK/blob/main/nlp_filings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTtxS-drfSas",
        "outputId": "07d3ffc7-2a96-411d-b779-e0458ea553f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 96.4M  100 96.4M    0     0  41.1M      0  0:00:02  0:00:02 --:--:-- 41.1M\n"
          ]
        }
      ],
      "source": [
        "!curl -H 'Authorization: token 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' -o fillings.json https://raw.githubusercontent.com/gajrajgchouhan/Extract_CIK/main/data/fillings.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEgJ5vgOhdlt",
        "outputId": "29e85d37-4cf4-4d9b-c40a-c2c0ae8dd415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 3.2 MB/s \n",
            "\u001b[?25hCollecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, vaderSentiment, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0 vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers vaderSentiment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8IHjnvKg28J",
        "outputId": "65b7d20c-0c0b-46da-cfa4-6f1c01081e8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import html\n",
        "from transformers import pipeline\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "summarizer = pipeline(\"summarization\")\n",
        "pd.get_option(\"display.max_columns\", None)\n",
        "nltk.download('punkt')\n",
        "\n",
        "filings = json.load(open(\"./fillings.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "AvhCbs4ZhTG2"
      },
      "outputs": [],
      "source": [
        "def clean1(text):\n",
        "    # First, for each of the following characters (or symbols), the function replaces it by an empty space on the text \n",
        "    for a_sign in [r'\\n', r'\\t', \"°\"]:\n",
        "        text = text.replace(a_sign, \" \")\n",
        "\n",
        "    # Here, for each punctution in a set of all existing punctuations, the function also replaces it by an empty space.\n",
        "    # for a_punc in string.punctuation:\n",
        "        # text = text.replace(a_punc, \" \")\n",
        "\n",
        "    # Morever, the fuction replaces '\\s+' (which represents a sequence of empty spaces) by an single empty space, avoiding unecessary spaces \n",
        "    # and also sets all letters to lower case to make it easier to analyse later.\n",
        "\n",
        "    text = BeautifulSoup(text).get_text()\n",
        "    text = re.sub('\\s+',\" \", text).lower()\n",
        "    \n",
        "    return text.strip() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJfhRd0yhv2j",
        "outputId": "b079e5fc-0f69-4389-928d-8e42bcc1f7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "section = Item 1\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0\n",
            "section = Item 1A\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0711\n",
            "section = Item 1B\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0\n",
            "section = Item 2\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.1363\n",
            "section = Item 3\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0099\n",
            "section = Item 4\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.2107\n",
            "section = Item 5\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.3512\n",
            "section = Item 6\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0\n",
            "section = Item 7\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.3971\n",
            "section = Item 7A\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.2268\n",
            "section = Item 8\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.2587\n",
            "section = Item 9\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t-0.2107\n",
            "section = Item 9A\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t-0.0793\n",
            "section = Item 9B\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t-0.1909\n",
            "section = Item 10\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0\n",
            "section = Item 11\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.0\n",
            "section = Item 12\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.379\n",
            "section = Item 13\n",
            "AVERAGE SENTIMENT FOR PARAGRAPH: \t0.1366\n"
          ]
        }
      ],
      "source": [
        "for filing in filings[10][\"fillings\"]:\n",
        "    for sections, text in filing['parsed'].items():\n",
        "        if len(text) == 0: continue\n",
        "        print(f\"section = {sections}\")\n",
        "        text = \" \".join(text)\n",
        "        text = clean1(text)\n",
        "        \n",
        "        # summarized = summarizer(text)\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        # simple example to tokenize paragraph into sentences for VADER\n",
        "        paragraphSentiments = 0.0\n",
        "        for sentence in sentences:\n",
        "            vs = analyzer.polarity_scores(sentence)\n",
        "            paragraphSentiments += vs[\"compound\"]\n",
        "        print(\"AVERAGE SENTIMENT FOR PARAGRAPH: \\t\" + str(round(paragraphSentiments / len(sentences), 4)))\n",
        "        \n",
        "    break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM9BDIyF+vxtU+kyWkZ7X+q",
      "include_colab_link": true,
      "name": "nlp_filings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
